{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cluster centers instead of full initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from botorch.test_functions.synthetic import Hartmann\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from baybe import Campaign\n",
    "from baybe.objective import Objective\n",
    "from baybe.parameters import NumericalContinuousParameter, TaskParameter\n",
    "from baybe.searchspace import SearchSpace\n",
    "from baybe.simulation import simulate_scenarios\n",
    "from baybe.targets import NumericalTarget\n",
    "from baybe.utils.plotting import create_example_plots\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The following settings are used to set up the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 6  # input dimensionality of the test function\n",
    "BATCH_SIZE = 1  # batch size of recommendations per DOE iteration\n",
    "N_MC_ITERATIONS = 30  # number of Monte Carlo runs\n",
    "N_DOE_ITERATIONS = 25  # number of DOE iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = Objective(\n",
    "    mode=\"SINGLE\", targets=[NumericalTarget(name=\"Target\", mode=\"MIN\")]\n",
    ")\n",
    "\n",
    "# The bounds of the search space are dictated by the test function:\n",
    "\n",
    "BOUNDS = Hartmann(dim=DIMENSION).bounds\n",
    "\n",
    "params = [\n",
    "    NumericalContinuousParameter(\n",
    "        name=f\"x{d}\",\n",
    "        bounds=(lower, upper),\n",
    "    )\n",
    "    for d, (lower, upper) in enumerate(BOUNDS.T)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a `TaskParameter` to encode the task context,\n",
    "which allows the model to establish a relationship between the training data and\n",
    "the data collected during the optimization process.\n",
    "Because we want to obtain recommendations only for the test function, we explicitly\n",
    "pass the `active_values` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_param = TaskParameter(\n",
    "    name=\"Function\",\n",
    "    values=[\"Test_Function\", \"Training_Function\"],\n",
    "    active_values=[\"Test_Function\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameters at hand, we can now create our search space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = [*params, task_param]\n",
    "searchspace = SearchSpace.from_product(parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Tasks\n",
    "\n",
    "To demonstrate the transfer learning mechanism, we consider the problem of optimizing\n",
    "the Hartmann function using training data from a slightly altered version. The used model is of course not aware of this relationship but needs to infer\n",
    "it from the data gathered during the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Due to a bug in the code, we need to adjust the `botorch_function_wrapper` function to ignore the `TaskParameter`. This is intended to be fixed in a future version of BayBE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def botorch_function_wrapper(test_function):\n",
    "\n",
    "    def wrapper(*x) -> float:\n",
    "        from torch import Tensor\n",
    "        # Cast the provided list of floats to a tensor.\n",
    "        if isinstance(x[0], str):\n",
    "            x_tensor = Tensor(x[1:])\n",
    "        else:\n",
    "            x_tensor = Tensor(x)\n",
    "        result = test_function.forward(x_tensor)\n",
    "        # We do not need to return a tuple here.\n",
    "        return float(result)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "# Once the bug is fixed, you should be able to use the already existing wrapper by uncommenting this line.\n",
    "# from baybe.utils.botorch_wrapper import botorch_function_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_hartmann(*x: float) -> float:\n",
    "    \"\"\"Calculate a shifted, scaled and noisy variant of the Hartman function.\"\"\"\n",
    "    noised_hartmann = Hartmann(dim=DIMENSION)\n",
    "    return 2.5 * botorch_function_wrapper(noised_hartmann)(x) + 3.25\n",
    "\n",
    "\n",
    "test_functions = {\n",
    "    \"Test_Function\": botorch_function_wrapper(Hartmann(dim=DIMENSION)),\n",
    "    \"Training_Function\": shifted_hartmann,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Loop\n",
    "\n",
    "We now simulate campaigns for different amounts of training data unveiled,\n",
    "to show the impact of transfer learning on the optimization performance.\n",
    "To average out and reduce statistical effects that might happen due to the random\n",
    "sampling of the provided data, we perform several Monte Carlo runs.\n",
    "\n",
    "The output of the following code is deleted to improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (30, 50, 100, 250):\n",
    "    results: list[pd.DataFrame] = []\n",
    "    # Produce a baseline by sampling \n",
    "    sampled_data = [searchspace.continuous.samples_random(n_points=n) for _ in range(N_MC_ITERATIONS)]\n",
    "    for num_clusters in (2, 5, 10, 15, 25):\n",
    "        initial_data = []\n",
    "        for i in range(N_MC_ITERATIONS):\n",
    "            data = sampled_data[i]\n",
    "            kmedoids = KMedoids(n_clusters=num_clusters).fit(data)\n",
    "            centers = pd.DataFrame(\n",
    "                data=kmedoids.cluster_centers_,\n",
    "                columns=(\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\"),\n",
    "            )\n",
    "            centers[\"Target\"] = centers.apply(\n",
    "                test_functions[\"Training_Function\"], axis=1\n",
    "            )\n",
    "            centers[\"Function\"] = \"Training_Function\"\n",
    "            initial_data.append(centers)\n",
    "\n",
    "        campaign = Campaign(searchspace=searchspace, objective=objective)\n",
    "        result_clustered = simulate_scenarios(\n",
    "            {f\"{num_clusters}\": campaign},\n",
    "            test_functions[\"Test_Function\"],\n",
    "            initial_data=initial_data,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            n_doe_iterations=N_DOE_ITERATIONS,\n",
    "        )\n",
    "        results.append(result_clustered)\n",
    "\n",
    "    # Provide a baseline by using all of the sampled data\n",
    "    campaign = Campaign(searchspace=searchspace, objective=objective)\n",
    "    for data in sampled_data:\n",
    "        data[\"Target\"] = data.apply(\n",
    "            test_functions[\"Training_Function\"], axis=1\n",
    "        )\n",
    "        data[\"Function\"] = \"Training_Function\"\n",
    "    result_baseline = simulate_scenarios(\n",
    "        {\"Baseline\": campaign},\n",
    "        test_functions[\"Test_Function\"],\n",
    "        initial_data=sampled_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_doe_iterations=N_DOE_ITERATIONS,\n",
    "    )\n",
    "    results.append(result_baseline)\n",
    "\n",
    "    results = pd.concat(results)\n",
    "\n",
    "    results.rename(columns={\"Scenario\": \"Num. clusters\"}, inplace=True)\n",
    "    path = Path(sys.path[0])\n",
    "    ax = sns.lineplot(\n",
    "        data=results,\n",
    "        marker=\"o\",\n",
    "        markersize=10,\n",
    "        x=\"Num_Experiments\",\n",
    "        y=\"Target_CumBest\",\n",
    "        hue=\"Num. clusters\",\n",
    "    )\n",
    "    create_example_plots(\n",
    "        ax=ax,\n",
    "        path=path,\n",
    "        base_name=f\"cluster_experiments_{n}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 30 initial points\n",
    "\n",
    "![image.png](results/cluster_experiments_30_dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 50 initial points\n",
    "\n",
    "![image.png](results/cluster_experiments_50_dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 100 initial points\n",
    "\n",
    "![image.png](results/cluster_experiments_100_dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 250 initial points\n",
    "\n",
    "![image.png](results/cluster_experiments_250_fallback.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The key result of this study is that choosing clusters instead of providing the full available data definitely has an influence on the performance and might be worth a more in-depth investigation.\n",
    "Interestingly, it seems like the baseline actually performs worst in our experiments, although it has the maximum number of points available. This could have several reasons, like the chosen function not being a suitable one for this exercise, or the availability of relatively many points distracting the optimizer. Also, it might be the case that it might take more iterations for the baseline to \"catch up\" and overtaking the experiments using clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baybe310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
